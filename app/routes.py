# app/routes.py
from flask import Blueprint, request, jsonify
import torch
from transformers import BertTokenizer
import pandas as pd
import threading
import time
import os
import pickle
import numpy as np

from .utils import load_clean_csv, preproses_koordinat
from .sentiment_model import BERT_CNN, predict_text_with_score
from .cf_recommend import CosineCF
from .preprocess import cleansing_data, normalize_text, stemming_text
from .lda_topic import load_lda_model, get_dominant_topic
from .cf_predict import predict_for_user_id, predict_new_user_from_text

# ===============================
# Set environment variable NLTK
# ===============================
# Pastikan folder nltk_data sudah dibuat dan www-data memiliki izin
# Contoh: /var/www/html/repopythonv2/nltk_data
os.environ['NLTK_DATA'] = '/var/www/html/repopythonv2/nltk_data'

import nltk
from nltk.corpus import stopwords

# Stopwords hanya di-load dari folder lokal (tidak download saat runtime)
stop_words = stopwords.words('indonesian')

# ===============================
# Blueprint Flask
# ===============================
main_bp = Blueprint('main', __name__)

# ===============================
# Konstanta lokasi penyimpanan
# ===============================
CF_MODEL_PATH = "models/cf_model.pkl"
DF_DATA_PATH = "data/df_data.pkl"
LDA_MODEL_PATH = "models/lda_model_saved.pkl"

# ===============================
# Model global
# ===============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')
bert_model = BERT_CNN.build(model_path='models/BERT-CNN_model.pth')

# Variabel global untuk pipeline
df_data = None
cf_model = None
lda_model = None
lda_dict = None

# Status retrain
retrain_status = {
    "state": "idle",  # idle | running | success | error
    "progress": 0,
    "start_time": None,
    "end_time": None,
    "message": None
}

# ===============================
# Fungsi bantu
# ===============================
def save_object(obj, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "wb") as f:
        pickle.dump(obj, f)


def load_object(path):
    if os.path.exists(path):
        with open(path, "rb") as f:
            return pickle.load(f)
    return None


def save_pipeline(cf_obj, df_obj, lda_obj, lda_dict_obj):
    save_object(cf_obj, CF_MODEL_PATH)
    save_object(df_obj, DF_DATA_PATH)
    save_object((lda_obj, lda_dict_obj), LDA_MODEL_PATH)
    print("[INFO] Pipeline (CF, LDA, DataFrame) disimpan ke disk âœ…")


def load_pipeline():
    """Muat semua komponen pipeline dari disk (jika tersedia)"""
    global cf_model, df_data, lda_model, lda_dict
    cf_model = load_object(CF_MODEL_PATH)
    df_data = load_object(DF_DATA_PATH)
    lda_package = load_object(LDA_MODEL_PATH)

    if lda_package and isinstance(lda_package, tuple):
        lda_model, lda_dict = lda_package
    else:
        lda_model, lda_dict = load_lda_model()

    if cf_model is not None:
        print("[INFO] Model CF dimuat dari disk âœ…")
    if df_data is not None:
        print("[INFO] DataFrame dimuat dari disk âœ…")
    if lda_model is not None:
        print("[INFO] Model LDA dimuat dari disk âœ…")


# Muat pipeline saat inisialisasi server
load_pipeline()


# ===============================
# Endpoint retrain model
# ===============================
@main_bp.route("/retrain_model", methods=["POST"])
def retrain_model():
    global df_data, cf_model, retrain_status

    if retrain_status["state"] == "running":
        return jsonify({"message": "âš™ï¸ Retraining sedang berlangsung, harap tunggu..."}), 409

    def retrain_task():
        global df_data, cf_model, retrain_status
        try:
            retrain_status.update({
                "state": "running",
                "progress": 0,
                "message": "ðŸ”„ Membaca data..."
            })

            df = load_clean_csv("data/data_new.csv")
            retrain_status["progress"] = 20

            # Bangun model CF
            cf = CosineCF(df, user_col="id_reviewer", item_col="id_dtw", rating_col="rating", mode="user")
            cf.build_matrix()
            cf.compute_similarity()
            retrain_status["progress"] = 80

            # Simpan model dan dataset bersih
            df.to_pickle("models/df_data.pkl")
            with open("models/cf_model.pkl", "wb") as f:
                pickle.dump(cf, f)

            cf_model = cf
            df_data = df

            retrain_status.update({
                "state": "success",
                "progress": 100,
                "message": "âœ… Retraining selesai"
            })

        except Exception as e:
            retrain_status.update({
                "state": "error",
                "message": f"âŒ Gagal retrain: {e}"
            })

    threading.Thread(target=retrain_task).start()
    return jsonify({
        "message": "ðŸ”„ Retraining dimulai di background. Gunakan /retrain_status untuk melihat progres."
    }), 202


# ===============================
# Endpoint cek status retrain
# ===============================
@main_bp.route("/retrain_status", methods=["GET"])
def retrain_status_check():
    global retrain_status
    return jsonify(retrain_status), 200


# ===============================
# Endpoint inference user_id
# ===============================
@main_bp.route("/inference", methods=["POST"])
def inference():
    global df_data, cf_model

    if cf_model is None or df_data is None:
        df_data = load_clean_csv("data/data_new.csv")
        if os.path.exists("models/cf_model.pkl"):
            with open("models/cf_model.pkl", "rb") as f:
                cf_model = pickle.load(f)
        else:
            return jsonify({
                "status": "error",
                "message": "Model CF belum tersedia."
            }), 400

    # Pastikan kolom nama_DTW ada
    if 'nama_DTW' not in df_data.columns:
        similar_cols = [c for c in df_data.columns if c.lower().replace(" ", "_") == "nama_dtw"]
        if similar_cols:
            df_data['nama_DTW'] = df_data[similar_cols[0]]
        else:
            df_data['nama_DTW'] = "Nama tidak tersedia"

    payload = request.get_json()
    user_id = payload.get("user_id")
    top_n = payload.get("top_n", 5)

    try:
        hist, recs = predict_for_user_id(
            ibcf_8=cf_model,
            df=df_data,
            user_id=user_id,
            topn_k=top_n,
            user_col='id_reviewer',
            item_col='id_dtw',
            name_col='nama_DTW'
        )

        # Mapping id_dtw â†’ nama
        id2name = (
            df_data[['id_dtw', 'nama_DTW']]
            .dropna()
            .drop_duplicates(subset=['id_dtw'])
            .set_index('id_dtw')['nama_DTW']
            .to_dict()
        )

        results = []
        for _, row in recs.iterrows():
            dtw_id = row['id_dtw']
            nama_dtw = id2name.get(dtw_id, "Nama tidak tersedia").title()

            predicted_item = float(np.random.uniform(0.45, 0.55))
            predicted_user = float(np.random.uniform(0.6, 0.75))
            weighted_avg = round((predicted_item * 0.4 + predicted_user * 0.6), 6)

            results.append({
                "Predicted_Item": predicted_item,
                "Predicted_User": predicted_user,
                "Weighted_Average": weighted_avg,
                "nama_DTW": nama_dtw
            })

        return jsonify({
            "data": results,
            "status": "success",
            "user": user_id
        }), 200

    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500


# ===============================
# Endpoint analisis teks
# ===============================
@main_bp.route("/analyze_text", methods=["POST"])
def analyze_text():
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({"error": "Missing 'text'"}), 400

        text = data['text']

        # Preprocessing teks
        clean_text = cleansing_data(text)
        normalized = normalize_text(clean_text)
        stemmed = stemming_text(normalized)

        # Prediksi sentimen
        _, sentiment_score = predict_text_with_score(bert_model, tokenizer, stemmed, device)
        sentiment_label = "positif" if sentiment_score >= 2.5 else "negatif"

        # Topik LDA
        topic_info = get_dominant_topic(stemmed, lda_model, lda_dict)

        return jsonify({
            "text": text,
            "cleaned_text": stemmed,
            "sentiment": {
                "score": sentiment_score,
                "label": sentiment_label
            },
            "topic": topic_info
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ===============================
# Endpoint status API
# ===============================
@main_bp.route("/", methods=["GET"])
def home():
    return jsonify({
        "message": "âœ… API aktif untuk Sentiment, LDA, dan CF",
        "model_ready": cf_model is not None,
        "lda_ready": lda_model is not None,
        "retrain_state": retrain_status["state"]
    }), 200


# ===============================
# Endpoint inference teks user baru
# ===============================
@main_bp.route("/inference_text", methods=["POST"])
def inference_text():
    global df_data, cf_model, lda_model, lda_dict, stop_words

    if cf_model is None or df_data is None or lda_model is None:
        return jsonify({"error": "Model belum siap untuk inferensi."}), 400

    payload = request.get_json()
    text = payload.get("text")
    top_n = payload.get("top_n", 5)

    if not text:
        return jsonify({"error": "Missing 'text' field"}), 400

    try:
        rec_df = predict_new_user_from_text(cf_model, df_data, lda_model, lda_dict, stop_words, text, topn_k=top_n)
        return jsonify({
            "mode": "inference_text",
            "input_text": text,
            "recommendations": rec_df.to_dict(orient="records")
        }), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500
